{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce4007af",
   "metadata": {
    "id": "ce4007af"
   },
   "source": [
    "# All Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b0ef5b1-2f69-450b-a8cd-0e48ef10a6bc",
   "metadata": {
    "id": "7b0ef5b1-2f69-450b-a8cd-0e48ef10a6bc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Swarup\\anaconda3\\envs\\condaenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\Swarup\\anaconda3\\envs\\condaenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy: No module named 'numpy.core._multiarray_umath' (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertForSequenceClassification, BertTokenizer\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcsv\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\condaenv\\lib\\site-packages\\transformers\\__init__.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[0;32m     29\u001b[0m     _LazyModule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     47\u001b[0m     logging,\n\u001b[0;32m     48\u001b[0m )\n\u001b[0;32m     51\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\condaenv\\lib\\site-packages\\transformers\\dependency_versions_check.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependency_versions_table\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[0;32m     25\u001b[0m pkgs_to_check_at_runtime \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyyaml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     38\u001b[0m ]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\condaenv\\lib\\site-packages\\transformers\\utils\\__init__.py:33\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     26\u001b[0m     add_code_sample_docstrings,\n\u001b[0;32m     27\u001b[0m     add_end_docstrings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m     replace_return_docstrings,\n\u001b[0;32m     32\u001b[0m )\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     34\u001b[0m     ContextManagers,\n\u001b[0;32m     35\u001b[0m     ExplicitEnum,\n\u001b[0;32m     36\u001b[0m     ModelOutput,\n\u001b[0;32m     37\u001b[0m     PaddingStrategy,\n\u001b[0;32m     38\u001b[0m     TensorType,\n\u001b[0;32m     39\u001b[0m     add_model_info_to_auto_map,\n\u001b[0;32m     40\u001b[0m     cached_property,\n\u001b[0;32m     41\u001b[0m     can_return_loss,\n\u001b[0;32m     42\u001b[0m     expand_dims,\n\u001b[0;32m     43\u001b[0m     find_labels,\n\u001b[0;32m     44\u001b[0m     flatten_dict,\n\u001b[0;32m     45\u001b[0m     infer_framework,\n\u001b[0;32m     46\u001b[0m     is_jax_tensor,\n\u001b[0;32m     47\u001b[0m     is_numpy_array,\n\u001b[0;32m     48\u001b[0m     is_tensor,\n\u001b[0;32m     49\u001b[0m     is_tf_symbolic_tensor,\n\u001b[0;32m     50\u001b[0m     is_tf_tensor,\n\u001b[0;32m     51\u001b[0m     is_torch_device,\n\u001b[0;32m     52\u001b[0m     is_torch_dtype,\n\u001b[0;32m     53\u001b[0m     is_torch_tensor,\n\u001b[0;32m     54\u001b[0m     reshape,\n\u001b[0;32m     55\u001b[0m     squeeze,\n\u001b[0;32m     56\u001b[0m     strtobool,\n\u001b[0;32m     57\u001b[0m     tensor_size,\n\u001b[0;32m     58\u001b[0m     to_numpy,\n\u001b[0;32m     59\u001b[0m     to_py_obj,\n\u001b[0;32m     60\u001b[0m     transpose,\n\u001b[0;32m     61\u001b[0m     working_or_temp_dir,\n\u001b[0;32m     62\u001b[0m )\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     64\u001b[0m     CLOUDFRONT_DISTRIB_PREFIX,\n\u001b[0;32m     65\u001b[0m     HF_MODULES_CACHE,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     91\u001b[0m     try_to_load_from_cache,\n\u001b[0;32m     92\u001b[0m )\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     94\u001b[0m     ACCELERATE_MIN_VERSION,\n\u001b[0;32m     95\u001b[0m     ENV_VARS_TRUE_AND_AUTO_VALUES,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    200\u001b[0m     torch_only_method,\n\u001b[0;32m    201\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\condaenv\\lib\\site-packages\\transformers\\utils\\generic.py:442\u001b[0m\n\u001b[0;32m    438\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[1;32m--> 442\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pytree\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_torch_pytree\u001b[39;00m\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_model_output_flatten\u001b[39m(output: ModelOutput) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[Any], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_pytree.Context\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output\u001b[38;5;241m.\u001b[39mvalues()), \u001b[38;5;28mlist\u001b[39m(output\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\condaenv\\lib\\site-packages\\torch\\__init__.py:1537\u001b[0m\n\u001b[0;32m   1535\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m testing \u001b[38;5;28;01mas\u001b[39;00m testing\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backends \u001b[38;5;28;01mas\u001b[39;00m backends\n\u001b[1;32m-> 1537\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __config__ \u001b[38;5;28;01mas\u001b[39;00m __config__\n\u001b[0;32m   1539\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __future__ \u001b[38;5;28;01mas\u001b[39;00m __future__\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\condaenv\\lib\\site-packages\\torch\\utils\\data\\__init__.py:21\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msampler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      4\u001b[0m     BatchSampler,\n\u001b[0;32m      5\u001b[0m     RandomSampler,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     WeightedRandomSampler,\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     12\u001b[0m     ChainDataset,\n\u001b[0;32m     13\u001b[0m     ConcatDataset,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m     random_split,\n\u001b[0;32m     20\u001b[0m )\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     22\u001b[0m     DFIterDataPipe,\n\u001b[0;32m     23\u001b[0m     DataChunk,\n\u001b[0;32m     24\u001b[0m     IterDataPipe,\n\u001b[0;32m     25\u001b[0m     MapDataPipe,\n\u001b[0;32m     26\u001b[0m )\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     DataLoader,\n\u001b[0;32m     29\u001b[0m     _DatasetKind,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m     default_convert,\n\u001b[0;32m     33\u001b[0m )\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DistributedSampler\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\condaenv\\lib\\site-packages\\torch\\utils\\data\\datapipes\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28miter\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mmap\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataframe\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\condaenv\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     IterableWrapperIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m IterableWrapper,\n\u001b[0;32m      3\u001b[0m )\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      5\u001b[0m     CollatorIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m Collator,\n\u001b[0;32m      6\u001b[0m     MapperIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m Mapper,\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinatorics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     SamplerIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m Sampler,\n\u001b[0;32m     10\u001b[0m     ShufflerIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m Shuffler,\n\u001b[0;32m     11\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\condaenv\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\utils.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IterDataPipe\n\u001b[0;32m      5\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterableWrapperIterDataPipe\u001b[39m\u001b[38;5;124m\"\u001b[39m, ]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIterableWrapperIterDataPipe\u001b[39;00m(IterDataPipe):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\condaenv\\lib\\site-packages\\torch\\utils\\data\\datapipes\\datapipe.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _DataPipeMeta, _IterDataPipeMeta\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_hook_iterator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _SnapshotState\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      8\u001b[0m     _deprecation_warning,\n\u001b[0;32m      9\u001b[0m     _iter_deprecated_functional_names,\n\u001b[0;32m     10\u001b[0m     _map_deprecated_functional_names,\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, IterableDataset\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\condaenv\\lib\\site-packages\\torch\\utils\\data\\datapipes\\utils\\common.py:13\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partial\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Dict, Iterable, List, Optional, Tuple, Union\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserialization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DILL_AVAILABLE\n\u001b[0;32m     15\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidate_input_col\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStreamWrapper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidate_pathname_binary_tuple\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     22\u001b[0m ]\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_input_col\u001b[39m(fn: Callable, input_col: Optional[Union[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m]]):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\condaenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\__init__.py:51\u001b[0m\n\u001b[0;32m     46\u001b[0m     python_exit_status \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     48\u001b[0m atexit\u001b[38;5;241m.\u001b[39mregister(_set_python_exit_flag)\n\u001b[1;32m---> 51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m worker, signal_handling, pin_memory, collate, fetch\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\condaenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:206\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;66;03m# For both ndarray and memmap (subclass of ndarray)\u001b[39;00m\n\u001b[1;32m--> 206\u001b[0m default_collate_fn_map[\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndarray\u001b[49m] \u001b[38;5;241m=\u001b[39m collate_numpy_array_fn\n\u001b[0;32m    207\u001b[0m \u001b[38;5;66;03m# See scalars hierarchy: https://numpy.org/doc/stable/reference/arrays.scalars.html\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;66;03m# Skip string scalars\u001b[39;00m\n\u001b[0;32m    209\u001b[0m default_collate_fn_map[(np\u001b[38;5;241m.\u001b[39mbool_, np\u001b[38;5;241m.\u001b[39mnumber, np\u001b[38;5;241m.\u001b[39mobject_)] \u001b[38;5;241m=\u001b[39m collate_numpy_scalar_fn\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'ndarray'"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5f0b9e-8425-46dd-a295-583802737599",
   "metadata": {
    "id": "5d5f0b9e-8425-46dd-a295-583802737599"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('ProsusAI/finbert')\n",
    "model = BertForSequenceClassification.from_pretrained('ProsusAI/finbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088e00b1-9c66-43a9-9af2-40c209bc8176",
   "metadata": {
    "id": "088e00b1-9c66-43a9-9af2-40c209bc8176"
   },
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# def read_multiline_string_from_csv(filename, column_name):\n",
    "#     with open(filename, 'r+', newline='', encoding=\"utf8\") as csvfile:\n",
    "#         reader = csv.DictReader(csvfile)\n",
    "#         header = reader.fieldnames\n",
    "#         header.append('score')  # Add 'score' as a new column header\n",
    "#         csvfile.seek(0)\n",
    "#         writer = csv.DictWriter(csvfile, fieldnames=header)\n",
    "\n",
    "#         # If 'score' column already exists, remove it temporarily\n",
    "#         if 'score' in header:\n",
    "#             header.remove('score')\n",
    "\n",
    "#         # Write the updated header\n",
    "#         writer.writeheader()\n",
    "\n",
    "#         for row in reader:\n",
    "#             txt = row[column_name]\n",
    "#             if txt is not None:\n",
    "#                 sentiment_score = finbert(txt)\n",
    "\n",
    "#                 # Add 'score' column with its value at the end of the row\n",
    "#                 row['score'] = sentiment_score\n",
    "#                 writer.writerow(row)\n",
    "#                 print(sentiment_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea93ff20",
   "metadata": {
    "id": "ea93ff20"
   },
   "source": [
    "# Finbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89faa705-319b-4253-980c-65243a840275",
   "metadata": {
    "id": "89faa705-319b-4253-980c-65243a840275"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def finbert(txt):\n",
    "    tokens = tokenizer.encode_plus(txt, add_special_tokens = False, return_tensors = 'pt')\n",
    "\n",
    "    input_id_chunks = tokens['input_ids'][0].split(510)\n",
    "    attention_mask_chunks = tokens['attention_mask'][0].split(510)\n",
    "\n",
    "    input_id_chunks, attention_mask_chunks = get_input_ids_and_attention_mask_chunk(tokens)\n",
    "\n",
    "    input_ids = torch.stack(input_id_chunks)\n",
    "    attention_mask = torch.stack(attention_mask_chunks)\n",
    "\n",
    "    input_dict = {\n",
    "        'input_ids' : input_ids.long(),\n",
    "        'attention_mask' : attention_mask.int()\n",
    "    }\n",
    "\n",
    "    outputs = model(**input_dict)\n",
    "    probabilities = torch.nn.functional.softmax(outputs[0], dim = -1 )\n",
    "    mean_probabilities = probabilities.mean(dim = 0)\n",
    "\n",
    "    return torch.argmax(mean_probabilities).item()\n",
    "\"\"\"\n",
    "\n",
    "def finbert(txt):\n",
    "    tokens = tokenizer.encode_plus(txt, add_special_tokens=False, return_tensors='pt')\n",
    "\n",
    "    input_id_chunks, attention_mask_chunks = get_input_ids_and_attention_mask_chunk(tokens)\n",
    "\n",
    "    input_ids = torch.stack(input_id_chunks)\n",
    "    attention_mask = torch.stack(attention_mask_chunks)\n",
    "\n",
    "    input_dict = {\n",
    "        'input_ids': input_ids.long(),\n",
    "        'attention_mask': attention_mask.int()\n",
    "    }\n",
    "\n",
    "    outputs = model(**input_dict)\n",
    "    probabilities = torch.nn.functional.softmax(outputs[0], dim=-1)\n",
    "    mean_probabilities = probabilities.mean(dim=0)\n",
    "\n",
    "    return torch.argmax(mean_probabilities).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eca8a41-89a3-4409-ab54-6186b8d38c1e",
   "metadata": {
    "id": "9eca8a41-89a3-4409-ab54-6186b8d38c1e"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def get_input_ids_and_attention_mask_chunk(tokens):\n",
    "    chunksize = 512\n",
    "    input_id_chunks = list(tokens['input_ids'][0].split(chunksize - 2))\n",
    "    attention_mask_chunks = list(tokens['attention_mask'][0].split(chunksize - 2))\n",
    "\n",
    "    for i in range(len(input_id_chunks)):\n",
    "        input_id_chunks[i] = torch.cat([\n",
    "            torch.tensor([101]), input_id_chunks[i], torch.tensor([102])\n",
    "        ])\n",
    "\n",
    "        attention_mask_chunks[i] = torch.cat([\n",
    "            torch.tensor([1]), attention_mask_chunks[i], torch.tensor([1])\n",
    "        ])\n",
    "\n",
    "        pad_length = chunksize - input_id_chunks[i].shape[0]\n",
    "\n",
    "        if pad_length > 0:\n",
    "            input_id_chunks[i] = torch.cat([\n",
    "                input_id_chunks[i], torch.Tensor([0] * pad_length)\n",
    "            ])\n",
    "            attention_mask_chunks[i] = torch.cat([\n",
    "                attention_mask_chunks[i], torch.Tensor([0] * pad_length)\n",
    "            ])\n",
    "\n",
    "    return input_id_chunks, attention_mask_chunks\n",
    "\"\"\"\n",
    "\n",
    "def get_input_ids_and_attention_mask_chunk(tokens):\n",
    "    chunksize = 512\n",
    "    input_id_chunks = list(tokens['input_ids'][0].split(chunksize - 2))\n",
    "    attention_mask_chunks = list(tokens['attention_mask'][0].split(chunksize - 2))\n",
    "\n",
    "    for i in range(len(input_id_chunks)):\n",
    "        input_id_chunks[i] = torch.cat([\n",
    "            torch.tensor([101]), input_id_chunks[i], torch.tensor([102])\n",
    "        ])\n",
    "\n",
    "        attention_mask_chunks[i] = torch.cat([\n",
    "            torch.tensor([1]), attention_mask_chunks[i], torch.tensor([1])\n",
    "        ])\n",
    "\n",
    "        pad_length = chunksize - input_id_chunks[i].shape[0]\n",
    "\n",
    "        if pad_length > 0:\n",
    "            input_id_chunks[i] = torch.cat([\n",
    "                input_id_chunks[i], torch.Tensor([0] * pad_length)\n",
    "            ])\n",
    "            attention_mask_chunks[i] = torch.cat([\n",
    "                attention_mask_chunks[i], torch.Tensor([0] * pad_length)\n",
    "            ])\n",
    "\n",
    "    return input_id_chunks, attention_mask_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2079bca-5662-40fe-bb4f-81a5d6d84a26",
   "metadata": {
    "id": "f2079bca-5662-40fe-bb4f-81a5d6d84a26"
   },
   "outputs": [],
   "source": [
    "# # Example usage:\n",
    "# import pandas as pd\n",
    "# filename = 'final_news_data.csv'  # Replace 'data.csv' with the path to your CSV file\n",
    "# column_name = 'Article Body'  # Replace 'column_name' with the name of the column containing the multi-line string\n",
    "# df = pd.read_csv(filename)\n",
    "# # multiline_string = read_multiline_string_from_csv(filename, column_name)\n",
    "# # Apply sentiment analysis and create a new column 'score'\n",
    "# column_data = df['column_name']\n",
    "# print(column_data)\n",
    "# # df['score'] = df['Article Body'].apply(finbert(column_name))\n",
    "# # # Save the updated DataFrame to a new CSV file\n",
    "# # output_file_path = 'output_file.csv'  # Replace with your desired output file path\n",
    "# # df.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55ebf0b",
   "metadata": {
    "id": "e55ebf0b"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# # Assuming you have a CSV file named 'your_data.csv'\n",
    "# file_path = 'final_news_data.csv'\n",
    "# # Load the data into a pandas DataFrame\n",
    "# df = pd.read_csv(file_path)\n",
    "# column_data = df['Article Body']\n",
    "# print(column_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ac3347-0d4d-4298-a6da-b723067c7bdc",
   "metadata": {
    "id": "15ac3347-0d4d-4298-a6da-b723067c7bdc"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# # Assuming you have a CSV file named 'your_data.csv'\n",
    "# file_path = 'merged_data1.csv'\n",
    "\n",
    "# # Load the data into a pandas DataFrame\n",
    "# df = pd.read_csv(file_path)\n",
    "\n",
    "# # Drop rows with NaN values in the 'Article Body' column\n",
    "# df_without_nan = df.dropna(subset=['Article Body'])\n",
    "\n",
    "# # Replace 'column_name' with the actual column name you want to extract\n",
    "# column_data_without_nan = df_without_nan['Article Body']\n",
    "\n",
    "# # Apply the finbert function to each row in the 'Article Body' column\n",
    "# df_without_nan['score'] = df_without_nan['Article Body'].apply(finbert)\n",
    "\n",
    "# # Save the DataFrame back to the CSV file with the new 'score' column\n",
    "# df_without_nan.to_csv(file_path, index=False)\n",
    "\n",
    "# # Print or use the extracted data as needed\n",
    "# print(df_without_nan['score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee31755a",
   "metadata": {
    "id": "ee31755a"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Assuming you have a CSV file named 'merged_data.csv'\n",
    "# file_path = 'RI_TOI.csv'\n",
    "\n",
    "# # Load the data into a pandas DataFrame\n",
    "# df = pd.read_csv(file_path)\n",
    "\n",
    "# # Drop rows with NaN values in the 'Article Body' column\n",
    "# df_without_nan = df.dropna(subset=['Article Body'])\n",
    "\n",
    "# # Replace 'column_name' with the actual column name you want to extract\n",
    "# column_data_without_nan = df_without_nan['Article Body']\n",
    "\n",
    "# # Apply the finbert function to each row in the 'Article Body' column\n",
    "# df_without_nan['score'] = df_without_nan['Article Body'].apply(finbert)\n",
    "\n",
    "# # Specify the new file path for the output CSV\n",
    "# output_file_path = 'RI_TOI_output_data.csv'\n",
    "\n",
    "# # Save the DataFrame to a new CSV file with the new 'score' column\n",
    "# df_without_nan.to_csv(output_file_path, index=False)\n",
    "\n",
    "# # Print or use the extracted data as needed\n",
    "# print(df_without_nan['score'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad618c5",
   "metadata": {
    "id": "6ad618c5"
   },
   "source": [
    "# Apply Finbert on Article Body and Generate Output CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c012efb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0c012efb",
    "outputId": "db837591-94d7-4a1e-dfa6-fb3d685000d1"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have a CSV file named 'merged_data.csv'\n",
    "file_path = 'merged_data_f.csv'\n",
    "\n",
    "# Load the data into a pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop rows with NaN values in the 'Article Body' column\n",
    "df_without_nan = df.dropna(subset=['Article Body'])\n",
    "\n",
    "# Replace 'column_name' with the actual column name you want to extract\n",
    "column_data_without_nan = df_without_nan['Article Body']\n",
    "\n",
    "# Create an empty list to store scores\n",
    "scores = []\n",
    "\n",
    "# Iterate over each row in the 'Article Body' column and calculate scores\n",
    "for text in tqdm(column_data_without_nan, desc=\"Calculating scores\", unit=\" articles\"):\n",
    "    score = finbert(text)\n",
    "    scores.append(score)\n",
    "\n",
    "# Add the scores to the DataFrame\n",
    "df_without_nan['score'] = scores\n",
    "\n",
    "# Specify the new file path for the output CSV\n",
    "output_file_path = 'merged_data_f_output.csv'\n",
    "\n",
    "# Save the DataFrame to a new CSV file with the new 'score' column\n",
    "df_without_nan.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Print or use the extracted data as needed\n",
    "print(df_without_nan['score'])\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have a CSV file named 'merged_data.csv'\n",
    "file_path = 'merged_data_f.csv'\n",
    "\n",
    "# Load the data into a pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Create an empty list to store processed rows\n",
    "processed_rows = []\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in tqdm(df.iterrows(), desc=\"Processing rows\", total=len(df), unit=\" rows\"):\n",
    "    # Check if any of the specified fields (e.g., 'Company', 'Article Body') are empty\n",
    "    if pd.isnull(row['Article Body']):\n",
    "        # If any field is empty, create a new row with all fields and sentiment score as -1\n",
    "        processed_row = row.copy()\n",
    "        processed_row['score'] = -1\n",
    "        processed_rows.append(processed_row)\n",
    "    else:\n",
    "        # If all required fields are present, proceed with sentiment analysis and scoring\n",
    "        text = row['Article Body']\n",
    "        score = finbert(text)\n",
    "        row['score'] = score\n",
    "        processed_rows.append(row)\n",
    "\n",
    "# Create a new DataFrame from the processed rows\n",
    "df_processed = pd.DataFrame(processed_rows)\n",
    "\n",
    "# Specify the new file path for the output CSV\n",
    "output_file_path = 'merged_data_f_output.csv'\n",
    "\n",
    "# Save the processed DataFrame to a new CSV file\n",
    "df_processed.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Print or use the extracted data as needed\n",
    "print(df_processed['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bb7518",
   "metadata": {
    "id": "65bb7518"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Assuming you have a CSV file named 'data.csv'\n",
    "# file_path = 'RI_TOI_output_data.csv'\n",
    "\n",
    "# # Load the data into a pandas DataFrame\n",
    "# df = pd.read_csv(file_path)\n",
    "\n",
    "# # Specify the column name for which you want to find non-null values\n",
    "# column_name = 'not_null'\n",
    "\n",
    "# # Get the indices of rows where the specified column is not null\n",
    "# non_null_indices = df[df[column_name].notnull()].index\n",
    "\n",
    "# # Print the row numbers with non-null values for the specified column\n",
    "# print(\"Row numbers with non-null values in column '{}':\".format(column_name))\n",
    "# print(non_null_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e09231",
   "metadata": {
    "id": "d6e09231"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f915845d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torch_kernel",
   "language": "python",
   "name": "torch_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
