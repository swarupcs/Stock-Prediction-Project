{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce4007af",
   "metadata": {
    "id": "ce4007af"
   },
   "source": [
    "# All Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b0ef5b1-2f69-450b-a8cd-0e48ef10a6bc",
   "metadata": {
    "id": "7b0ef5b1-2f69-450b-a8cd-0e48ef10a6bc"
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d5f0b9e-8425-46dd-a295-583802737599",
   "metadata": {
    "id": "5d5f0b9e-8425-46dd-a295-583802737599"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('ProsusAI/finbert')\n",
    "model = BertForSequenceClassification.from_pretrained('ProsusAI/finbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088e00b1-9c66-43a9-9af2-40c209bc8176",
   "metadata": {
    "id": "088e00b1-9c66-43a9-9af2-40c209bc8176"
   },
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# def read_multiline_string_from_csv(filename, column_name):\n",
    "#     with open(filename, 'r+', newline='', encoding=\"utf8\") as csvfile:\n",
    "#         reader = csv.DictReader(csvfile)\n",
    "#         header = reader.fieldnames\n",
    "#         header.append('score')  # Add 'score' as a new column header\n",
    "#         csvfile.seek(0)\n",
    "#         writer = csv.DictWriter(csvfile, fieldnames=header)\n",
    "\n",
    "#         # If 'score' column already exists, remove it temporarily\n",
    "#         if 'score' in header:\n",
    "#             header.remove('score')\n",
    "\n",
    "#         # Write the updated header\n",
    "#         writer.writeheader()\n",
    "\n",
    "#         for row in reader:\n",
    "#             txt = row[column_name]\n",
    "#             if txt is not None:\n",
    "#                 sentiment_score = finbert(txt)\n",
    "\n",
    "#                 # Add 'score' column with its value at the end of the row\n",
    "#                 row['score'] = sentiment_score\n",
    "#                 writer.writerow(row)\n",
    "#                 print(sentiment_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea93ff20",
   "metadata": {
    "id": "ea93ff20"
   },
   "source": [
    "# Finbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89faa705-319b-4253-980c-65243a840275",
   "metadata": {
    "id": "89faa705-319b-4253-980c-65243a840275"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def finbert(txt):\n",
    "    tokens = tokenizer.encode_plus(txt, add_special_tokens = False, return_tensors = 'pt')\n",
    "\n",
    "    input_id_chunks = tokens['input_ids'][0].split(510)\n",
    "    attention_mask_chunks = tokens['attention_mask'][0].split(510)\n",
    "\n",
    "    input_id_chunks, attention_mask_chunks = get_input_ids_and_attention_mask_chunk(tokens)\n",
    "\n",
    "    input_ids = torch.stack(input_id_chunks)\n",
    "    attention_mask = torch.stack(attention_mask_chunks)\n",
    "\n",
    "    input_dict = {\n",
    "        'input_ids' : input_ids.long(),\n",
    "        'attention_mask' : attention_mask.int()\n",
    "    }\n",
    "\n",
    "    outputs = model(**input_dict)\n",
    "    probabilities = torch.nn.functional.softmax(outputs[0], dim = -1 )\n",
    "    mean_probabilities = probabilities.mean(dim = 0)\n",
    "\n",
    "    return torch.argmax(mean_probabilities).item()\n",
    "\"\"\"\n",
    "\n",
    "def finbert(txt):\n",
    "    tokens = tokenizer.encode_plus(txt, add_special_tokens=False, return_tensors='pt')\n",
    "\n",
    "    input_id_chunks, attention_mask_chunks = get_input_ids_and_attention_mask_chunk(tokens)\n",
    "\n",
    "    input_ids = torch.stack(input_id_chunks)\n",
    "    attention_mask = torch.stack(attention_mask_chunks)\n",
    "\n",
    "    input_dict = {\n",
    "        'input_ids': input_ids.long(),\n",
    "        'attention_mask': attention_mask.int()\n",
    "    }\n",
    "\n",
    "    outputs = model(**input_dict)\n",
    "    probabilities = torch.nn.functional.softmax(outputs[0], dim=-1)\n",
    "    mean_probabilities = probabilities.mean(dim=0)\n",
    "\n",
    "    return torch.argmax(mean_probabilities).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9eca8a41-89a3-4409-ab54-6186b8d38c1e",
   "metadata": {
    "id": "9eca8a41-89a3-4409-ab54-6186b8d38c1e"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def get_input_ids_and_attention_mask_chunk(tokens):\n",
    "    chunksize = 512\n",
    "    input_id_chunks = list(tokens['input_ids'][0].split(chunksize - 2))\n",
    "    attention_mask_chunks = list(tokens['attention_mask'][0].split(chunksize - 2))\n",
    "\n",
    "    for i in range(len(input_id_chunks)):\n",
    "        input_id_chunks[i] = torch.cat([\n",
    "            torch.tensor([101]), input_id_chunks[i], torch.tensor([102])\n",
    "        ])\n",
    "\n",
    "        attention_mask_chunks[i] = torch.cat([\n",
    "            torch.tensor([1]), attention_mask_chunks[i], torch.tensor([1])\n",
    "        ])\n",
    "\n",
    "        pad_length = chunksize - input_id_chunks[i].shape[0]\n",
    "\n",
    "        if pad_length > 0:\n",
    "            input_id_chunks[i] = torch.cat([\n",
    "                input_id_chunks[i], torch.Tensor([0] * pad_length)\n",
    "            ])\n",
    "            attention_mask_chunks[i] = torch.cat([\n",
    "                attention_mask_chunks[i], torch.Tensor([0] * pad_length)\n",
    "            ])\n",
    "\n",
    "    return input_id_chunks, attention_mask_chunks\n",
    "\"\"\"\n",
    "\n",
    "def get_input_ids_and_attention_mask_chunk(tokens):\n",
    "    chunksize = 512\n",
    "    input_id_chunks = list(tokens['input_ids'][0].split(chunksize - 2))\n",
    "    attention_mask_chunks = list(tokens['attention_mask'][0].split(chunksize - 2))\n",
    "\n",
    "    for i in range(len(input_id_chunks)):\n",
    "        input_id_chunks[i] = torch.cat([\n",
    "            torch.tensor([101]), input_id_chunks[i], torch.tensor([102])\n",
    "        ])\n",
    "\n",
    "        attention_mask_chunks[i] = torch.cat([\n",
    "            torch.tensor([1]), attention_mask_chunks[i], torch.tensor([1])\n",
    "        ])\n",
    "\n",
    "        pad_length = chunksize - input_id_chunks[i].shape[0]\n",
    "\n",
    "        if pad_length > 0:\n",
    "            input_id_chunks[i] = torch.cat([\n",
    "                input_id_chunks[i], torch.Tensor([0] * pad_length)\n",
    "            ])\n",
    "            attention_mask_chunks[i] = torch.cat([\n",
    "                attention_mask_chunks[i], torch.Tensor([0] * pad_length)\n",
    "            ])\n",
    "\n",
    "    return input_id_chunks, attention_mask_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2079bca-5662-40fe-bb4f-81a5d6d84a26",
   "metadata": {
    "id": "f2079bca-5662-40fe-bb4f-81a5d6d84a26"
   },
   "outputs": [],
   "source": [
    "# # Example usage:\n",
    "# import pandas as pd\n",
    "# filename = 'final_news_data.csv'  # Replace 'data.csv' with the path to your CSV file\n",
    "# column_name = 'Article Body'  # Replace 'column_name' with the name of the column containing the multi-line string\n",
    "# df = pd.read_csv(filename)\n",
    "# # multiline_string = read_multiline_string_from_csv(filename, column_name)\n",
    "# # Apply sentiment analysis and create a new column 'score'\n",
    "# column_data = df['column_name']\n",
    "# print(column_data)\n",
    "# # df['score'] = df['Article Body'].apply(finbert(column_name))\n",
    "# # # Save the updated DataFrame to a new CSV file\n",
    "# # output_file_path = 'output_file.csv'  # Replace with your desired output file path\n",
    "# # df.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e55ebf0b",
   "metadata": {
    "id": "e55ebf0b"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# # Assuming you have a CSV file named 'your_data.csv'\n",
    "# file_path = 'final_news_data.csv'\n",
    "# # Load the data into a pandas DataFrame\n",
    "# df = pd.read_csv(file_path)\n",
    "# column_data = df['Article Body']\n",
    "# print(column_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15ac3347-0d4d-4298-a6da-b723067c7bdc",
   "metadata": {
    "id": "15ac3347-0d4d-4298-a6da-b723067c7bdc"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# # Assuming you have a CSV file named 'your_data.csv'\n",
    "# file_path = 'merged_data1.csv'\n",
    "\n",
    "# # Load the data into a pandas DataFrame\n",
    "# df = pd.read_csv(file_path)\n",
    "\n",
    "# # Drop rows with NaN values in the 'Article Body' column\n",
    "# df_without_nan = df.dropna(subset=['Article Body'])\n",
    "\n",
    "# # Replace 'column_name' with the actual column name you want to extract\n",
    "# column_data_without_nan = df_without_nan['Article Body']\n",
    "\n",
    "# # Apply the finbert function to each row in the 'Article Body' column\n",
    "# df_without_nan['score'] = df_without_nan['Article Body'].apply(finbert)\n",
    "\n",
    "# # Save the DataFrame back to the CSV file with the new 'score' column\n",
    "# df_without_nan.to_csv(file_path, index=False)\n",
    "\n",
    "# # Print or use the extracted data as needed\n",
    "# print(df_without_nan['score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee31755a",
   "metadata": {
    "id": "ee31755a"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Assuming you have a CSV file named 'merged_data.csv'\n",
    "# file_path = 'RI_TOI.csv'\n",
    "\n",
    "# # Load the data into a pandas DataFrame\n",
    "# df = pd.read_csv(file_path)\n",
    "\n",
    "# # Drop rows with NaN values in the 'Article Body' column\n",
    "# df_without_nan = df.dropna(subset=['Article Body'])\n",
    "\n",
    "# # Replace 'column_name' with the actual column name you want to extract\n",
    "# column_data_without_nan = df_without_nan['Article Body']\n",
    "\n",
    "# # Apply the finbert function to each row in the 'Article Body' column\n",
    "# df_without_nan['score'] = df_without_nan['Article Body'].apply(finbert)\n",
    "\n",
    "# # Specify the new file path for the output CSV\n",
    "# output_file_path = 'RI_TOI_output_data.csv'\n",
    "\n",
    "# # Save the DataFrame to a new CSV file with the new 'score' column\n",
    "# df_without_nan.to_csv(output_file_path, index=False)\n",
    "\n",
    "# # Print or use the extracted data as needed\n",
    "# print(df_without_nan['score'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad618c5",
   "metadata": {
    "id": "6ad618c5"
   },
   "source": [
    "# Apply Finbert on Article Body and Generate Output CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c012efb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0c012efb",
    "outputId": "db837591-94d7-4a1e-dfa6-fb3d685000d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:   0%|          | 0/1559 [00:00<?, ? rows/s]Token indices sequence length is longer than the specified maximum sequence length for this model (758 > 512). Running this sequence through the model will result in indexing errors\n",
      "Processing rows: 100%|██████████| 1559/1559 [30:37<00:00,  1.18s/ rows] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       1\n",
      "1      -1\n",
      "2      -1\n",
      "3      -1\n",
      "4      -1\n",
      "       ..\n",
      "1554    2\n",
      "1555    2\n",
      "1556   -1\n",
      "1557    2\n",
      "1558   -1\n",
      "Name: score, Length: 1559, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have a CSV file named 'merged_data.csv'\n",
    "file_path = 'merged_data_f.csv'\n",
    "\n",
    "# Load the data into a pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop rows with NaN values in the 'Article Body' column\n",
    "df_without_nan = df.dropna(subset=['Article Body'])\n",
    "\n",
    "# Replace 'column_name' with the actual column name you want to extract\n",
    "column_data_without_nan = df_without_nan['Article Body']\n",
    "\n",
    "# Create an empty list to store scores\n",
    "scores = []\n",
    "\n",
    "# Iterate over each row in the 'Article Body' column and calculate scores\n",
    "for text in tqdm(column_data_without_nan, desc=\"Calculating scores\", unit=\" articles\"):\n",
    "    score = finbert(text)\n",
    "    scores.append(score)\n",
    "\n",
    "# Add the scores to the DataFrame\n",
    "df_without_nan['score'] = scores\n",
    "\n",
    "# Specify the new file path for the output CSV\n",
    "output_file_path = 'merged_data_f_output.csv'\n",
    "\n",
    "# Save the DataFrame to a new CSV file with the new 'score' column\n",
    "df_without_nan.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Print or use the extracted data as needed\n",
    "print(df_without_nan['score'])\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have a CSV file named 'merged_data.csv'\n",
    "file_path = 'merged_data_f.csv'\n",
    "\n",
    "# Load the data into a pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Create an empty list to store processed rows\n",
    "processed_rows = []\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in tqdm(df.iterrows(), desc=\"Processing rows\", total=len(df), unit=\" rows\"):\n",
    "    # Check if any of the specified fields (e.g., 'Company', 'Article Body') are empty\n",
    "    if pd.isnull(row['Article Body']):\n",
    "        # If any field is empty, create a new row with all fields and sentiment score as -1\n",
    "        processed_row = row.copy()\n",
    "        processed_row['score'] = -1\n",
    "        processed_rows.append(processed_row)\n",
    "    else:\n",
    "        # If all required fields are present, proceed with sentiment analysis and scoring\n",
    "        text = row['Article Body']\n",
    "        score = finbert(text)\n",
    "        row['score'] = score\n",
    "        processed_rows.append(row)\n",
    "\n",
    "# Create a new DataFrame from the processed rows\n",
    "df_processed = pd.DataFrame(processed_rows)\n",
    "\n",
    "# Specify the new file path for the output CSV\n",
    "output_file_path = 'merged_data_f_output.csv'\n",
    "\n",
    "# Save the processed DataFrame to a new CSV file\n",
    "df_processed.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Print or use the extracted data as needed\n",
    "print(df_processed['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65bb7518",
   "metadata": {
    "id": "65bb7518"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Assuming you have a CSV file named 'data.csv'\n",
    "# file_path = 'RI_TOI_output_data.csv'\n",
    "\n",
    "# # Load the data into a pandas DataFrame\n",
    "# df = pd.read_csv(file_path)\n",
    "\n",
    "# # Specify the column name for which you want to find non-null values\n",
    "# column_name = 'not_null'\n",
    "\n",
    "# # Get the indices of rows where the specified column is not null\n",
    "# non_null_indices = df[df[column_name].notnull()].index\n",
    "\n",
    "# # Print the row numbers with non-null values for the specified column\n",
    "# print(\"Row numbers with non-null values in column '{}':\".format(column_name))\n",
    "# print(non_null_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e09231",
   "metadata": {
    "id": "d6e09231"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fed10e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "condaenv",
   "language": "python",
   "name": "condaenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
